{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Wrangling Lab**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will perform data wrangling tasks to prepare raw data for analysis. Data wrangling involves cleaning, transforming, and organizing data into a structured format suitable for analysis. This lab focuses on tasks like identifying inconsistencies, encoding categorical variables, and feature transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab, you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify and remove inconsistent data entries.\n",
    "\n",
    "- Encode categorical variables for analysis.\n",
    "\n",
    "- Handle missing values using multiple imputation strategies.\n",
    "\n",
    "- Apply feature scaling and transformation techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intsall the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.7 MB 3.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.6/8.7 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.4/8.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.2/8.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.6/8.7 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.0-cp313-cp313-win_amd64.whl (38.4 MB)\n",
      "   ---------------------------------------- 0.0/38.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 4.5/38.4 MB 21.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 10.2/38.4 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 18.6/38.4 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.5/38.4 MB 32.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.9/38.4 MB 33.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.4/38.4 MB 31.4 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.1 scipy-1.16.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import the necessary module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.1 Import necessary libraries and load the dataset.</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the dataset is loaded correctly by displaying the first few rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Stack Overflow survey data\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explore the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>2.1 Summarize the dataset by displaying the column data types, counts, and missing values.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65437 entries, 0 to 65436\n",
      "Columns: 114 entries, ResponseId to JobSat\n",
      "dtypes: float64(13), int64(1), object(100)\n",
      "memory usage: 56.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Summarizing the dataset by displaying the column data types, counts, and missing values\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>2.2 Generate basic statistics for numerical columns.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ResponseId      CompTotal       WorkExp  JobSatPoints_1  \\\n",
      "count  65437.000000   3.374000e+04  29658.000000    29324.000000   \n",
      "mean   32719.000000  2.963841e+145     11.466957       18.581094   \n",
      "std    18890.179119  5.444117e+147      9.168709       25.966221   \n",
      "min        1.000000   0.000000e+00      0.000000        0.000000   \n",
      "25%    16360.000000   6.000000e+04      4.000000        0.000000   \n",
      "50%    32719.000000   1.100000e+05      9.000000       10.000000   \n",
      "75%    49078.000000   2.500000e+05     16.000000       22.000000   \n",
      "max    65437.000000  1.000000e+150     50.000000      100.000000   \n",
      "\n",
      "       JobSatPoints_4  JobSatPoints_5  JobSatPoints_6  JobSatPoints_7  \\\n",
      "count    29393.000000    29411.000000    29450.000000     29448.00000   \n",
      "mean         7.522140       10.060857       24.343232        22.96522   \n",
      "std         18.422661       21.833836       27.089360        27.01774   \n",
      "min          0.000000        0.000000        0.000000         0.00000   \n",
      "25%          0.000000        0.000000        0.000000         0.00000   \n",
      "50%          0.000000        0.000000       20.000000        15.00000   \n",
      "75%          5.000000       10.000000       30.000000        30.00000   \n",
      "max        100.000000      100.000000      100.000000       100.00000   \n",
      "\n",
      "       JobSatPoints_8  JobSatPoints_9  JobSatPoints_10  JobSatPoints_11  \\\n",
      "count    29456.000000    29456.000000     29450.000000     29445.000000   \n",
      "mean        20.278165       16.169432        10.955713         9.953948   \n",
      "std         26.108110       24.845032        22.906263        21.775652   \n",
      "min          0.000000        0.000000         0.000000         0.000000   \n",
      "25%          0.000000        0.000000         0.000000         0.000000   \n",
      "50%         10.000000        5.000000         0.000000         0.000000   \n",
      "75%         25.000000       20.000000        10.000000        10.000000   \n",
      "max        100.000000      100.000000       100.000000       100.000000   \n",
      "\n",
      "       ConvertedCompYearly        JobSat  \n",
      "count         2.343500e+04  29126.000000  \n",
      "mean          8.615529e+04      6.935041  \n",
      "std           1.867570e+05      2.088259  \n",
      "min           1.000000e+00      0.000000  \n",
      "25%           3.271200e+04      6.000000  \n",
      "50%           6.500000e+04      7.000000  \n",
      "75%           1.079715e+05      8.000000  \n",
      "max           1.625660e+07     10.000000  \n"
     ]
    }
   ],
   "source": [
    "# Generating basic statistics for numerical columns\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identifying and Removing Inconsistencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>3.1 Identify inconsistent or irrelevant entries in specific columns (e.g., Country).</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in 'Country' column:\n",
      "['United States of America'\n",
      " 'United Kingdom of Great Britain and Northern Ireland' 'Canada' 'Norway'\n",
      " 'Uzbekistan' 'Serbia' 'Poland' 'Philippines' 'Bulgaria' 'Switzerland'\n",
      " 'India' 'Germany' 'Ireland' 'Italy' 'Ukraine' 'Australia' 'Brazil'\n",
      " 'Japan' 'Austria' 'Iran, Islamic Republic of...' 'France' 'Saudi Arabia'\n",
      " 'Romania' 'Turkey' 'Nepal' 'Algeria' 'Sweden' 'Netherlands' 'Croatia'\n",
      " 'Pakistan' 'Czech Republic' 'Republic of North Macedonia' 'Finland'\n",
      " 'Slovakia' 'Russian Federation' 'Greece' 'Israel' 'Belgium' 'Mexico'\n",
      " 'United Republic of Tanzania' 'Hungary' 'Argentina' 'Portugal'\n",
      " 'Sri Lanka' 'Latvia' 'China' 'Singapore' 'Lebanon' 'Spain' 'South Africa'\n",
      " 'Lithuania' 'Viet Nam' 'Dominican Republic' 'Indonesia' 'Kosovo'\n",
      " 'Morocco' 'Taiwan' 'Georgia' 'San Marino' 'Tunisia' 'Bangladesh'\n",
      " 'Nigeria' 'Liechtenstein' 'Denmark' 'Ecuador' 'Malaysia' 'Albania'\n",
      " 'Azerbaijan' 'Chile' 'Ghana' 'Peru' 'Bolivia' 'Egypt' 'Luxembourg'\n",
      " 'Montenegro' 'Cyprus' 'Paraguay' 'Kazakhstan' 'Slovenia' 'Jordan'\n",
      " 'Venezuela, Bolivarian Republic of...' 'Costa Rica' 'Jamaica' 'Thailand'\n",
      " 'Nicaragua' 'Myanmar' 'Republic of Korea' 'Rwanda'\n",
      " 'Bosnia and Herzegovina' 'Benin' 'El Salvador' 'Zimbabwe' 'Afghanistan'\n",
      " 'Estonia' 'Malta' 'Uruguay' 'Belarus' 'Colombia' 'Republic of Moldova'\n",
      " 'Isle of Man' 'Nomadic' 'New Zealand' 'Palestine' 'Armenia'\n",
      " 'United Arab Emirates' 'Maldives' 'Ethiopia' 'Fiji' 'Guatemala' 'Uganda'\n",
      " 'Turkmenistan' 'Mauritius' 'Kenya' 'Cuba' 'Gabon' 'Bahamas' 'South Korea'\n",
      " 'Iceland' 'Honduras' 'Hong Kong (S.A.R.)'\n",
      " \"Lao People's Democratic Republic\" 'Mongolia' 'Cambodia' 'Madagascar'\n",
      " 'Angola' 'Democratic Republic of the Congo' 'Syrian Arab Republic' 'Iraq'\n",
      " 'Namibia' 'Senegal' 'Kyrgyzstan' 'Zambia' 'Swaziland' \"Côte d'Ivoire\"\n",
      " 'Kuwait' 'Tajikistan' 'Burundi' 'Trinidad and Tobago' 'Mauritania'\n",
      " 'Sierra Leone' 'Panama' 'Somalia' 'North Korea' 'Dominica' 'Guyana'\n",
      " 'Togo' 'Oman' 'Barbados' 'Andorra'\n",
      " \"Democratic People's Republic of Korea\" 'Qatar' 'Sudan' 'Cameroon'\n",
      " 'Papua New Guinea' 'Bahrain' 'Yemen' 'Malawi' 'Burkina Faso'\n",
      " 'Congo, Republic of the...' 'Botswana' 'Guinea-Bissau' 'Mozambique'\n",
      " 'Central African Republic' 'Equatorial Guinea' 'Suriname' 'Belize'\n",
      " 'Libyan Arab Jamahiriya' 'Cape Verde' 'Brunei Darussalam' 'Bhutan'\n",
      " 'Guinea' 'Niger' 'Antigua and Barbuda' 'Mali' 'Samoa' 'Lesotho'\n",
      " 'Saint Kitts and Nevis' 'Monaco' 'Micronesia, Federated States of...'\n",
      " 'Haiti' nan 'Nauru' 'Liberia' 'Chad' 'Djibouti' 'Solomon Islands']\n",
      "\n",
      "Value counts for 'Country' column:\n",
      "Country\n",
      "United States of America                                11095\n",
      "Germany                                                  4947\n",
      "India                                                    4231\n",
      "United Kingdom of Great Britain and Northern Ireland     3224\n",
      "Ukraine                                                  2672\n",
      "                                                        ...  \n",
      "Micronesia, Federated States of...                          1\n",
      "Nauru                                                       1\n",
      "Chad                                                        1\n",
      "Djibouti                                                    1\n",
      "Solomon Islands                                             1\n",
      "Name: count, Length: 185, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identifying inconsistent or irrelevant entries in specific columns\n",
    "\n",
    "# Identify unique values in the 'Country' column\n",
    "unique_countries = df['Country'].unique()\n",
    "\n",
    "# Display the unique values\n",
    "print(\"\\nUnique values in 'Country' column:\")\n",
    "print(unique_countries)\n",
    "\n",
    "# You can also check value counts to see the distribution and potential inconsistencies\n",
    "print(\"\\nValue counts for 'Country' column:\")\n",
    "print(df['Country'].value_counts())\n",
    "\n",
    "# To identify specific inconsistencies, you might need to examine the unique_countries list\n",
    "# and look for variations in spelling or irrelevant entries.\n",
    "# For example, if 'USA' and 'United States' are both present, they might need to be standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>3.2 Standardize entries in columns like Country or EdLevel by mapping inconsistent values to a consistent format.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in 'EdLevel' column:\n",
      "['Primary/elementary school'\n",
      " 'Bachelor’s degree (B.A., B.S., B.Eng., etc.)'\n",
      " 'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)'\n",
      " 'Some college/university study without earning a degree'\n",
      " 'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)'\n",
      " 'Professional degree (JD, MD, Ph.D, Ed.D, etc.)'\n",
      " 'Associate degree (A.A., A.S., etc.)' 'Something else' nan]\n",
      "\n",
      "Value counts for 'EdLevel' column:\n",
      "EdLevel\n",
      "Bachelor’s degree (B.A., B.S., B.Eng., etc.)                                          24942\n",
      "Master’s degree (M.A., M.S., M.Eng., MBA, etc.)                                       15557\n",
      "Some college/university study without earning a degree                                 7651\n",
      "Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)     5793\n",
      "Professional degree (JD, MD, Ph.D, Ed.D, etc.)                                         2970\n",
      "Associate degree (A.A., A.S., etc.)                                                    1793\n",
      "Primary/elementary school                                                              1146\n",
      "Something else                                                                          932\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for 'EdLevel' column after standardization:\n",
      "EdLevel\n",
      "Bachelor’s degree (B.A., B.S., B.Eng., etc.)                                          24942\n",
      "Master’s degree (M.A., M.S., M.Eng., MBA, etc.)                                       15557\n",
      "Some college/university study                                                          7651\n",
      "Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)     5793\n",
      "Professional degree (JD, MD, Ph.D, Ed.D, etc.)                                         2970\n",
      "Associate degree (A.A., A.S., etc.)                                                    1793\n",
      "Primary/elementary school                                                              1146\n",
      "Something else                                                                          932\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Standardizing entries in columns like Country or EdLevel by mapping inconsistent values to a consistent format\n",
    "\n",
    "# Identify unique values in 'EdLevel' column\n",
    "unique_edlevel = df['EdLevel'].unique()\n",
    "print(\"\\nUnique values in 'EdLevel' column:\")\n",
    "print(unique_edlevel)\n",
    "\n",
    "# Display value counts for 'EdLevel' column\n",
    "print(\"\\nValue counts for 'EdLevel' column:\")\n",
    "print(df['EdLevel'].value_counts())\n",
    "\n",
    "# Create a mapping dictionary for 'EdLevel' to standardize values\n",
    "# Replace 'Some college/university study without earning a degree' and\n",
    "# 'Associate degree' with a consistent value if needed.\n",
    "# For example, if you want to group similar levels:\n",
    "edlevel_mapping = {\n",
    "  'Some college/university study without earning a degree': 'Some college/university study',\n",
    "  'Associate degree': 'Some college/university study'\n",
    "  # Add other mappings as needed based on inspection of unique values\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'EdLevel' column\n",
    "df['EdLevel'] = df['EdLevel'].replace(edlevel_mapping)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nValue counts for 'EdLevel' column after standardization:\")\n",
    "print(df['EdLevel'].value_counts())\n",
    "\n",
    "# Similarly, you would do this for 'Country' if inconsistencies were found.\n",
    "# Example mapping for 'Country' (assuming you found 'USA' and 'United States' as issues):\n",
    "# country_mapping = {\n",
    "#     'USA': 'United States',\n",
    "#     'United States of America': 'United States'\n",
    "#     # Add other mappings as needed\n",
    "# }\n",
    "# df['Country'] = df['Country'].replace(country_mapping)\n",
    "# print(\"\\nValue counts for 'Country' column after standardization:\")\n",
    "# print(df['Country'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Encoding Categorical Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>4.1 Encode the Employment column using one-hot encoding.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age RemoteWork  \\\n",
      "0           1  I am a developer by profession  Under 18 years old     Remote   \n",
      "1           2  I am a developer by profession     35-44 years old     Remote   \n",
      "2           3  I am a developer by profession     45-54 years old     Remote   \n",
      "3           4           I am learning to code     18-24 years old        NaN   \n",
      "4           5  I am a developer by profession     18-24 years old        NaN   \n",
      "\n",
      "    Check                                   CodingActivities  \\\n",
      "0  Apples                                              Hobby   \n",
      "1  Apples  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Apples  Hobby;Contribute to open-source projects;Other...   \n",
      "3  Apples                                                NaN   \n",
      "4  Apples                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3                      Some college/university study   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  \\\n",
      "0                                                NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial   \n",
      "4  Technical documentation;Blogs;Written Tutorial...   \n",
      "\n",
      "                                             TechDoc  ...  \\\n",
      "0                                                NaN  ...   \n",
      "1  API document(s) and/or SDK document(s);User gu...  ...   \n",
      "2  API document(s) and/or SDK document(s);User gu...  ...   \n",
      "3                                                NaN  ...   \n",
      "4  API document(s) and/or SDK document(s);User gu...  ...   \n",
      "\n",
      "  Employment_Student, full-time;Not employed, but looking for work;Not employed, and not looking for work;Student, part-time  \\\n",
      "0                                              False                                                                           \n",
      "1                                              False                                                                           \n",
      "2                                              False                                                                           \n",
      "3                                              False                                                                           \n",
      "4                                              False                                                                           \n",
      "\n",
      "  Employment_Student, full-time;Not employed, but looking for work;Retired  \\\n",
      "0                                              False                         \n",
      "1                                              False                         \n",
      "2                                              False                         \n",
      "3                                              False                         \n",
      "4                                              False                         \n",
      "\n",
      "  Employment_Student, full-time;Not employed, but looking for work;Student, part-time  \\\n",
      "0                                              False                                    \n",
      "1                                              False                                    \n",
      "2                                              False                                    \n",
      "3                                              False                                    \n",
      "4                                              False                                    \n",
      "\n",
      "  Employment_Student, full-time;Retired  \\\n",
      "0                                 False   \n",
      "1                                 False   \n",
      "2                                 False   \n",
      "3                                 False   \n",
      "4                                 False   \n",
      "\n",
      "  Employment_Student, full-time;Student, part-time  \\\n",
      "0                                            False   \n",
      "1                                            False   \n",
      "2                                            False   \n",
      "3                                            False   \n",
      "4                                            False   \n",
      "\n",
      "  Employment_Student, full-time;Student, part-time;Employed, part-time  \\\n",
      "0                                              False                     \n",
      "1                                              False                     \n",
      "2                                              False                     \n",
      "3                                              False                     \n",
      "4                                              False                     \n",
      "\n",
      "  Employment_Student, full-time;Student, part-time;Retired  \\\n",
      "0                                              False         \n",
      "1                                              False         \n",
      "2                                              False         \n",
      "3                                              False         \n",
      "4                                              False         \n",
      "\n",
      "  Employment_Student, part-time  \\\n",
      "0                         False   \n",
      "1                         False   \n",
      "2                         False   \n",
      "3                         False   \n",
      "4                         False   \n",
      "\n",
      "  Employment_Student, part-time;Employed, part-time  \\\n",
      "0                                             False   \n",
      "1                                             False   \n",
      "2                                             False   \n",
      "3                                             False   \n",
      "4                                             False   \n",
      "\n",
      "  Employment_Student, part-time;Retired  \n",
      "0                                 False  \n",
      "1                                 False  \n",
      "2                                 False  \n",
      "3                                 False  \n",
      "4                                 False  \n",
      "\n",
      "[5 rows x 223 columns]\n"
     ]
    }
   ],
   "source": [
    "# Encoding the Employment column using one-hot encoding\n",
    "\n",
    "import pandas as pd\n",
    "# Encode the 'Employment' column using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['Employment'], prefix='Employment')\n",
    "\n",
    "# Display the first few rows to verify the new columns\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>5.1 Identify columns with the highest number of missing values.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns with the highest number of missing values:\n",
      "AINextMuch less integrated    64289\n",
      "AINextLess integrated         63082\n",
      "AINextNo change               52939\n",
      "AINextMuch more integrated    51999\n",
      "EmbeddedAdmired               48704\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identifying columns with the highest number of missing values\n",
    "\n",
    "# Identify columns with the highest number of missing values\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# Sort columns by the number of missing values in descending order\n",
    "columns_with_most_missing = missing_values_count.sort_values(ascending=False)\n",
    "\n",
    "# Display the columns with the highest number of missing values\n",
    "print(\"\\nColumns with the highest number of missing values:\")\n",
    "print(columns_with_most_missing.head()) # You can adjust .head() to see more columns if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>5.2 Impute missing values in numerical columns (e.g., `ConvertedCompYearly`) with the mean or median.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after mean imputation:\n",
      "ResponseId                       0\n",
      "YearsCodePro                  2906\n",
      "CompTotal                        0\n",
      "WorkExp                          0\n",
      "JobSatPoints_1                   0\n",
      "JobSatPoints_4                   0\n",
      "JobSatPoints_5                   0\n",
      "JobSatPoints_6                   0\n",
      "JobSatPoints_7                   0\n",
      "JobSatPoints_8                   0\n",
      "JobSatPoints_9                   0\n",
      "JobSatPoints_10                  0\n",
      "JobSatPoints_11                  0\n",
      "ConvertedCompYearly              0\n",
      "JobSat                           0\n",
      "ConvertedCompYearly_Scaled       0\n",
      "ConvertedCompYearly_Log          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputing missing values in all numerical columns with the mean .\n",
    "\n",
    "# Select only numerical columns\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Impute missing values in numerical columns with the mean\n",
    "for col in numerical_cols:\n",
    "  if df[col].isnull().any():\n",
    "    mean_val = df[col].mean()\n",
    "    df[col].fillna(mean_val)\n",
    "\n",
    "# Verify that missing values in numerical columns are filled\n",
    "print(\"\\nMissing values after mean imputation:\")\n",
    "print(df[numerical_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>5.3 Impute missing values in categorical columns (e.g., `RemoteWork`) with the most frequent value.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after mode imputation:\n",
      "MainBranch              0\n",
      "Age                     0\n",
      "RemoteWork              0\n",
      "Check                   0\n",
      "CodingActivities        0\n",
      "                       ..\n",
      "ProfessionalQuestion    0\n",
      "Industry                0\n",
      "SurveyLength            0\n",
      "SurveyEase              0\n",
      "ExperienceLevel         0\n",
      "Length: 99, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values in all categorical columns with the most frequent value\n",
    "\n",
    "# Select only categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute missing values in categorical columns with the most frequent value\n",
    "for col in categorical_cols:\n",
    "  if df[col].isnull().any():\n",
    "    mode_val = df[col].mode()[0]  # .mode() returns a Series, so [0] gets the first mode\n",
    "    df[col].fillna(mode_val)\n",
    "\n",
    "# Verify that missing values in categorical columns are filled\n",
    "print(\"\\nMissing values after mode imputation:\")\n",
    "print(df[categorical_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature Scaling and Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>6.1 Apply Min-Max Scaling to normalize the `ConvertedCompYearly` column.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ConvertedCompYearly  ConvertedCompYearly_Scaled\n",
      "0         86155.287263                      0.0053\n",
      "1         86155.287263                      0.0053\n",
      "2         86155.287263                      0.0053\n",
      "3         86155.287263                      0.0053\n",
      "4         86155.287263                      0.0053\n",
      "\n",
      "Min of scaled 'ConvertedCompYearly': 0.0\n",
      "Max of scaled 'ConvertedCompYearly': 1.0\n"
     ]
    }
   ],
   "source": [
    "# Min-Max Scaling to normalize the `ConvertedCompYearly` column\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Reshape the 'ConvertedCompYearly' column to be 2D, as required by MinMaxScaler\n",
    "converted_comp_yearly_reshaped = df[['ConvertedCompYearly']]\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "df['ConvertedCompYearly_Scaled'] = scaler.fit_transform(converted_comp_yearly_reshaped)\n",
    "\n",
    "# Display the first few rows to verify the new scaled column\n",
    "print(df[['ConvertedCompYearly', 'ConvertedCompYearly_Scaled']].head())\n",
    "\n",
    "# You can also verify the min and max of the scaled column\n",
    "print(\"\\nMin of scaled 'ConvertedCompYearly':\", df['ConvertedCompYearly_Scaled'].min())\n",
    "print(\"Max of scaled 'ConvertedCompYearly':\", df['ConvertedCompYearly_Scaled'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>6.2 Log-transform the ConvertedCompYearly column to reduce skewness.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ConvertedCompYearly  ConvertedCompYearly_Log\n",
      "0         86155.287263                11.363918\n",
      "1         86155.287263                11.363918\n",
      "2         86155.287263                11.363918\n",
      "3         86155.287263                11.363918\n",
      "4         86155.287263                11.363918\n"
     ]
    }
   ],
   "source": [
    "# Log-transform the ConvertedCompYearly column to reduce skewness\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Apply log transformation to the 'ConvertedCompYearly' column\n",
    "# Add a small constant (e.g., 1) to avoid log(0) if 0 values are present\n",
    "df['ConvertedCompYearly_Log'] = np.log1p(df['ConvertedCompYearly'])\n",
    "\n",
    "# Display the first few rows to verify the new log-transformed column\n",
    "print(df[['ConvertedCompYearly', 'ConvertedCompYearly_Log']].head())\n",
    "\n",
    "# You can also plot a histogram of the original and log-transformed columns to visualize the effect\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# df['ConvertedCompYearly'].hist(bins=50)\n",
    "# plt.title('Original ConvertedCompYearly')\n",
    "# plt.subplot(1, 2, 2)\n",
    "# df['ConvertedCompYearly_Log'].hist(bins=50)\n",
    "# plt.title('Log-transformed ConvertedCompYearly')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>7.1 Create a new column `ExperienceLevel` based on the `YearsCodePro` column:</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   YearsCodePro ExperienceLevel\n",
      "0           2.0     Entry-level\n",
      "1          17.0          Senior\n",
      "2          27.0          Senior\n",
      "3           2.0     Entry-level\n",
      "4           2.0     Entry-level\n",
      "\n",
      "Value counts for 'ExperienceLevel' column:\n",
      "ExperienceLevel\n",
      "Entry-level    27942\n",
      "Mid-level      21477\n",
      "Senior         13112\n",
      "Unknown         2906\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a new column `ExperienceLevel` based on the `YearsCodePro` column\n",
    "\n",
    "import pandas as pd\n",
    "# Define the function to categorize experience level\n",
    "def categorize_experience(years):\n",
    "  if pd.isna(years):\n",
    "    return 'Unknown'\n",
    "  elif years < 5:\n",
    "    return 'Entry-level'\n",
    "  elif years >= 5 and years < 15:\n",
    "    return 'Mid-level'\n",
    "  else:\n",
    "    return 'Senior'\n",
    "\n",
    "# Convert 'YearsCodePro' to numeric, coercing errors to NaN\n",
    "df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n",
    "\n",
    "# Apply the function to create the new column\n",
    "df['ExperienceLevel'] = df['YearsCodePro'].apply(categorize_experience)\n",
    "\n",
    "# Display the first few rows with the new column\n",
    "print(df[['YearsCodePro', 'ExperienceLevel']].head())\n",
    "\n",
    "# Display value counts for the new column to see the distribution\n",
    "print(\"\\nValue counts for 'ExperienceLevel' column:\")\n",
    "print(df['ExperienceLevel'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you:\n",
    "\n",
    "- Explored the dataset to identify inconsistencies and missing values.\n",
    "\n",
    "- Encoded categorical variables for analysis.\n",
    "\n",
    "- Handled missing values using imputation techniques.\n",
    "\n",
    "- Normalized and transformed numerical data to prepare it for analysis.\n",
    "\n",
    "- Engineered a new feature to enhance data interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "prev_pub_hash": "1e8e234f19fd098e27b0518a87f18de690e1c51f1d3263d5690927d19971251e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
